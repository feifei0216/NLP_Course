{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用CNN模型搭建文本分类器\n",
    "\n",
    "数据集：intent分类数据集 https://arxiv.org/abs/1909.02027\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用意图分类数据，每一行数据由如下格式组成：\n",
    "\n",
    "[domain]  [intent]  [utterance]\n",
    "\n",
    "该数据集共有150个类别标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------train data------\n",
      "travel\tinternational_visa\tdo i need a visa to go to cancun\n",
      "travel\tplug_type\tshould i bring my socket converter while traveling to england\n",
      "banking\trouting_number\tlet me know my routing number\n",
      "small_talk\twhat_are_your_hobbies\twhat are some things that you enjoy\n",
      "work\tpto_request_status\thas my vacation time been signed off on\n",
      "------------labels----------\n",
      "150\n",
      "auto_&_commute@current_location\n",
      "auto_&_commute@directions\n",
      "auto_&_commute@distance\n",
      "auto_&_commute@gas\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "train_file = \"/home/data/tmp/nlp5_text_cnn/ind_train\"\n",
    "val_file = \"/home/data/tmp/nlp5_text_cnn/ind_val\"\n",
    "ind_label_list = \"/home/data/tmp/nlp5_text_cnn/ind_label_list\"\n",
    "\n",
    "# train data\n",
    "print('------------train data------')\n",
    "with open(train_file) as f:\n",
    "    res = f.readlines()\n",
    "    random.shuffle(res)\n",
    "    for i in res[:5]:\n",
    "        print(i.strip())\n",
    "\n",
    "print('------------labels----------')\n",
    "with open(ind_label_list) as f:\n",
    "    res = f.readlines()\n",
    "    print(len(res))\n",
    "    for i in res[:4]:\n",
    "        print(i.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了简单起见，我们直接使用了transformers库中所提供的tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'man', 'who', 'changed', 'china']\n",
      "[1996, 2158, 2040, 2904, 2859]\n",
      "the man who changed china\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "bert_path = \"/home/data/tmp/nlp5_text_cnn\"\n",
    "tokz = BertTokenizer.from_pretrained(bert_path)\n",
    "utter = \"The man who changed China\"\n",
    "print(tokz.tokenize(utter))\n",
    "print(tokz.convert_tokens_to_ids(tokz.tokenize(utter)))\n",
    "print(tokz.decode(tokz.convert_tokens_to_ids(tokz.tokenize(utter))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取标签，并生成标签词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ind_label_list) as f:\n",
    "    res = [i.strip().lower() for i in f.readlines() if len(i.strip()) != 0]\n",
    "label2index = dict(zip(res, range(len(res))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义Dataset\n",
    "\n",
    "pytorch程序中最重要的就是根据自己的数据特点定义数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import json\n",
    "\n",
    "\n",
    "class ClsDataset(Dataset):\n",
    "    def __init__(self, paths, tokz, intent2id, max_lengths=100):\n",
    "        self.tokz = tokz\n",
    "        self.max_lengths = max_lengths\n",
    "        self.intent2id = intent2id \n",
    "        self.data = self.make_dataset(paths, tokz, intent2id, max_lengths)\n",
    "\n",
    "    def make_dataset(self, paths, tokz, intent2id, max_lengths):\n",
    "        dataset = []\n",
    "        print('reading ind data from {}'.format(paths))\n",
    "        for path in paths:\n",
    "            with open(path, 'r', encoding='utf8') as f:\n",
    "                lines = [i.strip().lower().split('\\t') for i in f.readlines() if len(i.strip()) != 0]\n",
    "                for line in lines:\n",
    "                    dataset.append([intent2id[line[0] + '@' + line[1]],\n",
    "                                    tokz.convert_tokens_to_ids(tokz.tokenize(line[2])[:max_lengths])])\n",
    "        print('{} data record loaded'.format(len(dataset)))\n",
    "        return dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label, text = self.data[idx]\n",
    "        # encode label here\n",
    "        text = [self.tokz.cls_token_id] + text + [self.tokz.sep_token_id]\n",
    "        return {\"text\": text, \"text_len\": len(text), \"label\": int(label)}\n",
    "\n",
    "\n",
    "# 自定义batch操作\n",
    "class PadBatchSeq:\n",
    "    def __init__(self, pad_id):\n",
    "        self.pad_id = pad_id\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        res = dict()\n",
    "        res['label'] = torch.LongTensor([i['label'] for i in batch])\n",
    "        res['text_len'] = torch.LongTensor([i['text_len'] for i in batch])\n",
    "        text_max_len = max([len(i['text']) for i in batch])\n",
    "        res['text'] = torch.LongTensor([i['text'] + [self.pad_id] * (text_max_len - len(i['text'])) for i in batch])\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试一下刚才所定义的数据集类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading ind data from ['/home/data/tmp/nlp5_text_cnn/ind_val']\n",
      "3000 data record loaded\n",
      "3000\n",
      "{'text': [101, 4863, 2129, 2079, 1045, 2131, 2047, 5427, 102], 'text_len': 9, 'label': 138}\n",
      "[CLS] explain how do i get new insurance [SEP]\n"
     ]
    }
   ],
   "source": [
    "tmp = ClsDataset([val_file], tokz, label2index)\n",
    "print(len(tmp))\n",
    "print(tmp[100])\n",
    "print(tokz.decode(tmp[100]['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们把句子传进模型的时候，我们是按照一个个 batch 穿进去的，也就是说，我们一次传入了好几个句子，而且每个batch中的句子必须是相同的长度。为了确保句子的长度相同，我们需要把不够长的句子补齐。这个行为在PadBatchSeq中控制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': tensor([[  101,  1999,  3009,  1010,  3113,  2033,  4826,  2003,  2056,  2129,\n",
      "           102,     0,     0],\n",
      "        [  101,  1999,  2413,  1010,  2129,  2079,  1045,  2360,  1010,  2156,\n",
      "          2017,  2101,   102],\n",
      "        [  101,  2129,  2079,  2017,  2360,  7592,  1999,  2887,   102,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  2129,  2079,  1045,  3198,  2055,  1996,  4633,  1999,  2822,\n",
      "           102,     0,     0],\n",
      "        [  101,  2129,  2064,  1045,  2360,  1000, 17542,  2026,  2344,  1000,\n",
      "          1999,  2413,   102]]), 'text_len': tensor([11, 13,  9, 11, 13]), 'label': tensor([115, 115, 115, 115, 115])}\n"
     ]
    }
   ],
   "source": [
    "Pad_class = PadBatchSeq(tokz.pad_token_id)\n",
    "print(Pad_class([tmp[i] for i in range(5)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义CNN模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把模型超参数定义在一起"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attrdict import AttrDict\n",
    "config={\"max_length\": 40,\n",
    "  \"embedding_size\": 768,\n",
    "  \"feature_size\": [256, 256, 256, 256],\n",
    "  \"kernel_size\": [1, 2, 3, 4],\n",
    "  \"fc_size\": [256, 256],\n",
    "  \"embedding_dropout\": 0.1,\n",
    "  \"dropout\": 0.1,\n",
    "  \"eval_steps\": 90,\n",
    "  \"lr\": 4e-5,\n",
    "  \"batch_size\": 60,\n",
    "  \"n_epochs\": 30,\n",
    "}\n",
    "config = AttrDict(config)\n",
    "\n",
    "# 如果没有GPU，那么就使用CPU\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "# 如果有GPU，那么就使用GPU\n",
    "device = torch.device('cuda', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class Dense(nn.Module):\n",
    "    def __init__(self, in_size, out_size, activation=F.relu):\n",
    "        super(Dense, self).__init__()\n",
    "        self.linear = nn.Linear(in_size, out_size)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.activation(self.linear(x))\n",
    "\n",
    "\n",
    "class CNNModule(nn.Module):\n",
    "    def __init__(self, num_cls, n_embeddings, embedding_size, padding_idx, embed_dropout,\n",
    "                 feature_size=[128, 128, 128], kernel_size=[2, 3, 4], fc_size=[265, 265], dropout=0.2):\n",
    "        super(CNNModule, self).__init__()\n",
    "        self.embeddings = nn.Embedding(n_embeddings, embedding_size, padding_idx=padding_idx)\n",
    "        self.embed_dropout = nn.Dropout(embed_dropout)\n",
    "\n",
    "        self.convs = nn.ModuleList([nn.Conv1d(embedding_size, fs, ks) for fs, ks in zip(feature_size, kernel_size)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        fc_size = list(fc_size)\n",
    "        fc_size = list(zip([sum(feature_size)] + fc_size[1:], fc_size))\n",
    "        self.fc = nn.ModuleList([Dense(i, j) for i, j in fc_size])\n",
    "        self.output_layer = nn.Linear(fc_size[-1][-1], num_cls)\n",
    "\n",
    "    def forward(self, x, x_len):\n",
    "        '''x: [bs, len], x_len: [bs]'''\n",
    "        x_embed = self.embeddings(x) # x_embed: [bs, len, embed_size]\n",
    "        x_embed = self.embed_dropout(x_embed)\n",
    "        mask = torch.arange(x_embed.shape[1], device=x_len.device)[None, :] < x_len[:, None]  # [bs, max_len]\n",
    "        x_embed = x_embed * mask.unsqueeze(2)  # x_embed: [bs, len, embed_size]\n",
    "        x_embed = x_embed.permute([0, 2, 1])   # [bs, embed_size, len]\n",
    "        x_embed = [conv(x_embed) for conv in self.convs]  # [(bs, fs, len), ...]\n",
    "        x_embed = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x_embed]  # [(bs, fs), ...]\n",
    "        x_embed = torch.cat(x_embed, 1)  # [bs, sum(fs)]\n",
    "        x_embed = self.dropout(x_embed)\n",
    "        for fc in self.fc:\n",
    "            x_embed = fc(x_embed)\n",
    "        x_embed = self.dropout(x_embed)\n",
    "        logits = self.output_layer(x_embed)\n",
    "        return logits   # [bs, logits]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载数据集以及定义DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading ind data from ['/home/data/tmp/nlp5_text_cnn/ind_train']\n",
      "15000 data record loaded\n",
      "reading ind data from ['/home/data/tmp/nlp5_text_cnn/ind_val']\n",
      "3000 data record loaded\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataset = ClsDataset([train_file], tokz, label2index, max_lengths=config.max_length)\n",
    "val_dataset = ClsDataset([val_file], tokz, label2index, max_lengths=config.max_length)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, sampler=torch.utils.data.RandomSampler(train_dataset), \n",
    "                                  batch_size=config.batch_size, collate_fn=PadBatchSeq(tokz.pad_token_id))\n",
    "val_dataloader = DataLoader(val_dataset, sampler=None, batch_size=config.batch_size, collate_fn=PadBatchSeq(tokz.pad_token_id))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义模型，并搬运到相应的设备上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNModule(len(label2index), len(tokz), config.embedding_size, tokz.pad_token_id, config.embedding_dropout,\n",
    "                  config.feature_size, config.kernel_size, config.fc_size, config.dropout)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义优化器，以及损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = Adam(model.parameters(), lr=config.lr, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------epoch 0----------\n",
      "step 90, train loss 4.995, train acc 1.30%, val loss 4.969, val acc 1.83%\n",
      "step 180, train loss 4.941, train acc 3.52%, val loss 4.897, val acc 4.80%\n",
      "---------epoch 1----------\n",
      "step 270, train loss 4.779, train acc 11.42%, val loss 4.775, val acc 12.40%\n",
      "step 360, train loss 4.666, train acc 12.24%, val loss 4.552, val acc 20.47%\n",
      "step 450, train loss 4.390, train acc 16.30%, val loss 4.213, val acc 21.80%\n",
      "---------epoch 2----------\n",
      "step 540, train loss 3.863, train acc 23.13%, val loss 3.844, val acc 24.90%\n",
      "step 630, train loss 3.666, train acc 26.07%, val loss 3.506, val acc 33.43%\n",
      "step 720, train loss 3.354, train acc 31.44%, val loss 3.198, val acc 38.43%\n",
      "---------epoch 3----------\n",
      "step 810, train loss 2.954, train acc 39.47%, val loss 2.904, val acc 43.73%\n",
      "step 900, train loss 2.776, train acc 42.06%, val loss 2.660, val acc 50.00%\n",
      "step 990, train loss 2.502, train acc 48.04%, val loss 2.439, val acc 52.73%\n",
      "---------epoch 4----------\n",
      "step 1080, train loss 2.271, train acc 52.85%, val loss 2.249, val acc 57.13%\n",
      "step 1170, train loss 2.102, train acc 54.98%, val loss 2.083, val acc 59.40%\n",
      "---------epoch 5----------\n",
      "step 1260, train loss 1.882, train acc 61.00%, val loss 1.933, val acc 61.33%\n",
      "step 1350, train loss 1.771, train acc 62.35%, val loss 1.813, val acc 63.97%\n",
      "step 1440, train loss 1.690, train acc 64.33%, val loss 1.704, val acc 66.40%\n",
      "---------epoch 6----------\n",
      "step 1530, train loss 1.512, train acc 66.61%, val loss 1.626, val acc 66.83%\n",
      "step 1620, train loss 1.450, train acc 69.35%, val loss 1.531, val acc 69.33%\n",
      "step 1710, train loss 1.386, train acc 70.11%, val loss 1.459, val acc 70.47%\n",
      "---------epoch 7----------\n",
      "step 1800, train loss 1.273, train acc 72.50%, val loss 1.399, val acc 71.17%\n",
      "step 1890, train loss 1.229, train acc 73.96%, val loss 1.343, val acc 71.87%\n",
      "step 1980, train loss 1.171, train acc 73.91%, val loss 1.290, val acc 73.50%\n",
      "---------epoch 8----------\n",
      "step 2070, train loss 1.098, train acc 76.93%, val loss 1.250, val acc 73.97%\n",
      "step 2160, train loss 1.054, train acc 78.17%, val loss 1.203, val acc 75.03%\n",
      "step 2250, train loss 1.030, train acc 77.57%, val loss 1.164, val acc 75.03%\n",
      "---------epoch 9----------\n",
      "step 2340, train loss 0.951, train acc 79.72%, val loss 1.129, val acc 76.43%\n",
      "step 2430, train loss 0.942, train acc 80.22%, val loss 1.098, val acc 76.90%\n",
      "---------epoch 10----------\n",
      "step 2520, train loss 0.864, train acc 82.92%, val loss 1.074, val acc 77.83%\n",
      "step 2610, train loss 0.861, train acc 82.93%, val loss 1.040, val acc 78.67%\n",
      "step 2700, train loss 0.827, train acc 83.87%, val loss 1.021, val acc 78.80%\n",
      "---------epoch 11----------\n",
      "step 2790, train loss 0.787, train acc 84.04%, val loss 0.995, val acc 79.27%\n",
      "step 2880, train loss 0.770, train acc 84.74%, val loss 0.975, val acc 79.73%\n",
      "step 2970, train loss 0.734, train acc 85.83%, val loss 0.952, val acc 79.77%\n",
      "---------epoch 12----------\n",
      "step 3060, train loss 0.710, train acc 86.67%, val loss 0.931, val acc 81.20%\n",
      "step 3150, train loss 0.687, train acc 87.41%, val loss 0.913, val acc 81.13%\n",
      "step 3240, train loss 0.697, train acc 86.22%, val loss 0.905, val acc 81.67%\n",
      "---------epoch 13----------\n",
      "step 3330, train loss 0.662, train acc 87.19%, val loss 0.884, val acc 82.10%\n",
      "step 3420, train loss 0.646, train acc 87.70%, val loss 0.864, val acc 82.47%\n",
      "---------epoch 14----------\n",
      "step 3510, train loss 0.567, train acc 89.83%, val loss 0.856, val acc 82.47%\n",
      "step 3600, train loss 0.617, train acc 88.83%, val loss 0.844, val acc 82.50%\n",
      "step 3690, train loss 0.573, train acc 89.65%, val loss 0.827, val acc 82.67%\n",
      "---------epoch 15----------\n",
      "step 3780, train loss 0.549, train acc 90.22%, val loss 0.819, val acc 83.07%\n",
      "step 3870, train loss 0.566, train acc 89.89%, val loss 0.808, val acc 83.53%\n",
      "step 3960, train loss 0.555, train acc 90.33%, val loss 0.795, val acc 83.73%\n",
      "---------epoch 16----------\n",
      "step 4050, train loss 0.530, train acc 90.20%, val loss 0.784, val acc 83.77%\n",
      "step 4140, train loss 0.518, train acc 91.22%, val loss 0.782, val acc 83.77%\n",
      "step 4230, train loss 0.508, train acc 90.80%, val loss 0.772, val acc 84.10%\n",
      "---------epoch 17----------\n",
      "step 4320, train loss 0.497, train acc 91.81%, val loss 0.768, val acc 84.03%\n",
      "step 4410, train loss 0.493, train acc 91.65%, val loss 0.758, val acc 84.20%\n",
      "step 4500, train loss 0.489, train acc 91.48%, val loss 0.748, val acc 84.20%\n",
      "---------epoch 18----------\n",
      "step 4590, train loss 0.464, train acc 92.48%, val loss 0.741, val acc 84.60%\n",
      "step 4680, train loss 0.477, train acc 92.17%, val loss 0.734, val acc 84.60%\n",
      "---------epoch 19----------\n",
      "step 4770, train loss 0.441, train acc 92.92%, val loss 0.730, val acc 85.20%\n",
      "step 4860, train loss 0.431, train acc 93.43%, val loss 0.729, val acc 84.53%\n",
      "step 4950, train loss 0.431, train acc 93.26%, val loss 0.714, val acc 85.10%\n",
      "---------epoch 20----------\n",
      "step 5040, train loss 0.414, train acc 94.25%, val loss 0.713, val acc 84.87%\n",
      "step 5130, train loss 0.431, train acc 93.50%, val loss 0.702, val acc 85.27%\n",
      "step 5220, train loss 0.420, train acc 93.94%, val loss 0.702, val acc 85.33%\n",
      "---------epoch 21----------\n",
      "step 5310, train loss 0.398, train acc 93.78%, val loss 0.701, val acc 85.30%\n",
      "step 5400, train loss 0.393, train acc 94.54%, val loss 0.696, val acc 85.40%\n",
      "step 5490, train loss 0.403, train acc 93.78%, val loss 0.686, val acc 85.97%\n",
      "---------epoch 22----------\n",
      "step 5580, train loss 0.379, train acc 94.29%, val loss 0.691, val acc 85.20%\n",
      "step 5670, train loss 0.385, train acc 94.57%, val loss 0.674, val acc 85.33%\n",
      "---------epoch 23----------\n",
      "step 5760, train loss 0.348, train acc 95.17%, val loss 0.673, val acc 85.70%\n",
      "step 5850, train loss 0.365, train acc 94.67%, val loss 0.677, val acc 85.50%\n",
      "step 5940, train loss 0.380, train acc 94.56%, val loss 0.670, val acc 85.77%\n",
      "---------epoch 24----------\n",
      "step 6030, train loss 0.328, train acc 95.44%, val loss 0.664, val acc 86.07%\n",
      "step 6120, train loss 0.339, train acc 95.59%, val loss 0.670, val acc 85.83%\n",
      "step 6210, train loss 0.371, train acc 94.72%, val loss 0.663, val acc 86.00%\n",
      "---------epoch 25----------\n",
      "step 6300, train loss 0.333, train acc 95.77%, val loss 0.659, val acc 85.77%\n",
      "step 6390, train loss 0.352, train acc 95.30%, val loss 0.659, val acc 86.30%\n",
      "step 6480, train loss 0.343, train acc 95.56%, val loss 0.655, val acc 85.93%\n",
      "---------epoch 26----------\n",
      "step 6570, train loss 0.324, train acc 96.00%, val loss 0.650, val acc 86.40%\n",
      "step 6660, train loss 0.329, train acc 95.52%, val loss 0.650, val acc 86.07%\n",
      "step 6750, train loss 0.332, train acc 95.61%, val loss 0.647, val acc 86.33%\n",
      "---------epoch 27----------\n",
      "step 6840, train loss 0.319, train acc 96.33%, val loss 0.648, val acc 86.17%\n",
      "step 6930, train loss 0.315, train acc 96.22%, val loss 0.644, val acc 86.47%\n",
      "---------epoch 28----------\n",
      "step 7020, train loss 0.310, train acc 97.08%, val loss 0.642, val acc 86.53%\n",
      "step 7110, train loss 0.306, train acc 96.46%, val loss 0.643, val acc 86.47%\n",
      "step 7200, train loss 0.322, train acc 95.41%, val loss 0.642, val acc 86.50%\n",
      "---------epoch 29----------\n",
      "step 7290, train loss 0.282, train acc 97.08%, val loss 0.640, val acc 86.00%\n",
      "step 7380, train loss 0.301, train acc 96.37%, val loss 0.639, val acc 86.13%\n",
      "step 7470, train loss 0.311, train acc 96.09%, val loss 0.639, val acc 86.07%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(config.n_epochs):\n",
    "    print('---------epoch {}----------'.format(epoch))\n",
    "    model.train()\n",
    "\n",
    "    loss, acc, step_count = 0, 0, 0\n",
    "    total = len(train_dataloader)\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        d_data = data\n",
    "\n",
    "        text, label = d_data['text'].to(device), d_data['label'].to(device)\n",
    "        text_len = d_data['text_len'].to(device)\n",
    "\n",
    "        outputs = model(text, text_len)\n",
    "        batch_loss = criterion(outputs, label)\n",
    "        batch_acc = (torch.argmax(outputs, dim=1) == label).float().mean()\n",
    "\n",
    "        batch_loss.backward()\n",
    "\n",
    "        loss += batch_loss.item()\n",
    "        acc += batch_acc.item()\n",
    "        step_count += 1\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        curr_step = optimizer.state[optimizer.param_groups[0][\"params\"][-1]][\"step\"]\n",
    "\n",
    "        if curr_step % config.eval_steps == 0:\n",
    "            \n",
    "            # eval\n",
    "            model.eval()\n",
    "\n",
    "            val_logits = []\n",
    "            val_label = []\n",
    "            for d_data in val_dataloader:\n",
    "                text, label = d_data['text'].to(device), d_data['label'].to(device)\n",
    "                text_len = d_data['text_len'].to(device)\n",
    "                outputs = model(text, text_len)\n",
    "                val_label.append(label)\n",
    "                val_logits.append(outputs)\n",
    "\n",
    "\n",
    "            val_logits = torch.cat(val_logits, dim=0)\n",
    "            val_label = torch.cat(val_label, dim=0)\n",
    "\n",
    "            val_loss = criterion(val_logits, val_label).float()\n",
    "            val_acc = (torch.argmax(val_logits, dim=1) == val_label).float().mean()\n",
    "            \n",
    "            loss /= step_count\n",
    "            acc /= step_count\n",
    "            print('step {}, train loss {:>4.3f}, train acc {:>4.2f}%, val loss {:>4.3f}, val acc {:>4.2f}%'.format(curr_step, loss, acc * 100, val_loss, val_acc * 100))\n",
    "            loss, acc, step_count = 0, 0, 0\n",
    "            model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading ind data from ['/home/data/tmp/nlp5_text_cnn/ind_val']\n",
      "3000 data record loaded\n",
      "test results:\n",
      "loss 0.6381175\n",
      "acc 0.86733335\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "test_file = \"/home/data/tmp/nlp5_text_cnn/ind_val\"\n",
    "test_dataset = ClsDataset([test_file], tokz, label2index, max_lengths=config.max_length)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=None, batch_size=config.batch_size, collate_fn=PadBatchSeq(tokz.pad_token_id))\n",
    "\n",
    "test_logits = []\n",
    "test_label = []\n",
    "model.eval()\n",
    "for d_data in test_dataloader:\n",
    "    text, label = d_data['text'].to(device), d_data['label'].to(device)\n",
    "    text_len = d_data['text_len'].to(device)\n",
    "    outputs = model(text, text_len)\n",
    "    test_label.append(label)\n",
    "    test_logits.append(outputs)\n",
    "\n",
    "model.train()\n",
    "test_logits = torch.cat(test_logits, dim=0)\n",
    "test_label = torch.cat(test_label, dim=0)\n",
    "\n",
    "test_loss = criterion(test_logits, test_label).cpu().detach().numpy()\n",
    "test_acc = (torch.argmax(test_logits, dim=1) == test_label).cpu().float().mean().numpy()\n",
    "print('test results:')\n",
    "print('loss', test_loss)\n",
    "print('acc', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
